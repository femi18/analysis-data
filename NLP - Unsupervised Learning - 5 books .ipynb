{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Unsupervised Learning - 5 Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\femis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\femis\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "symbolic link created for C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Linking successful\n",
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this modeling we would use Poems by Blake and Stories by Bryant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "poems = gutenberg.raw('blake-poems.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Chapter indicator is idiosyncratic\n",
    "poems = re.sub(r'Chapter \\d+', '', poems)\n",
    "stories = re.sub(r'CHAPTER .*', '', stories)\n",
    "sense = re.sub(r'CHAPTER .*', '', sense)\n",
    "busterbrown = re.sub(r'CHAPTER .*', '', busterbrown)\n",
    "ball = re.sub(r'CHAPTER .*', '', ball)\n",
    "    \n",
    "poems = text_cleaner(poems[:int(len(poems)/10)])\n",
    "stories = text_cleaner(stories[:int(len(stories)/10)])\n",
    "sense = text_cleaner(sense[:int(len(sense)/10)])\n",
    "busterbrown = text_cleaner(busterbrown[:int(len(busterbrown)/10)])\n",
    "ball = text_cleaner(ball[:int(len(ball)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text:\n",
      " SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping dow\n",
      "Cleaned text:\n",
      " TWO LITTLE RIDDLES IN RHYME There's a garden that I ken, Full of little gentlemen; Little caps of bl\n",
      "Cleaned text:\n",
      " The family of Dashwood had long been settled in Sussex. Their estate was large, and their residence \n",
      "Cleaned text:\n",
      " I BUSTER BEAR GOES FISHING Buster Bear yawned as he lay on his comfortable bed of leaves and watched\n",
      "Cleaned text:\n",
      " I. A DISCUSSION SOMEWHAT IN THE AIR The flying ship of Professor Lucifer sang through the skies like\n"
     ]
    }
   ],
   "source": [
    "print('Cleaned text:\\n', poems[0:100])\n",
    "print('Cleaned text:\\n', stories[0:100])\n",
    "print('Cleaned text:\\n', sense[0:100])\n",
    "print('Cleaned text:\\n', busterbrown[0:100])\n",
    "print('Cleaned text:\\n', ball[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "poems_doc = nlp(poems)\n",
    "stories_doc = nlp(stories)\n",
    "sense_doc = nlp(sense)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "ball_doc = nlp(ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(INNOCENCE, INTRODUCTION, Piping, down, the, v...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(wild)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(,, Piping, songs, of, pleasant, glee, ,, On, ...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\", Pipe, a, song, about, a, Lamb, !, \")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(So, I, piped, with, merry, cheer, .)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(\")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Piper, ,, pipe, that, song, again, ;)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(\", So, I, piped, :, he, wept, to, hear, .)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(\")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...  Blake\n",
       "1  (INNOCENCE, INTRODUCTION, Piping, down, the, v...  Blake\n",
       "2                                             (wild)  Blake\n",
       "3  (,, Piping, songs, of, pleasant, glee, ,, On, ...  Blake\n",
       "4           (\", Pipe, a, song, about, a, Lamb, !, \")  Blake\n",
       "5              (So, I, piped, with, merry, cheer, .)  Blake\n",
       "6                                                (\")  Blake\n",
       "7             (Piper, ,, pipe, that, song, again, ;)  Blake\n",
       "8        (\", So, I, piped, :, he, wept, to, hear, .)  Blake\n",
       "9                                                (\")  Blake"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "poems_sents = [[sent, \"Blake\"] for sent in poems_doc.sents]\n",
    "stories_sents = [[sent, \"Bryant\"] for sent in stories_doc.sents]\n",
    "sense_sents = [[sent, \"austen\"] for sent in sense_doc.sents]\n",
    "busterbrown_sents = [[sent, \"burgess\"] for sent in busterbrown_doc.sents]\n",
    "ball_sents = [[sent, \"chesterton\"] for sent in ball_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(poems_sents + stories_sents +sense_sents + busterbrown_sents + ball_sents)\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "poemswords = bag_of_words(poems_doc)\n",
    "storieswords = bag_of_words(stories_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "ballwords = bag_of_words(ball_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(poemswords + storieswords + sensewords + busterbrownwords + ballwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annuity</th>\n",
       "      <th>interminable</th>\n",
       "      <th>clearly</th>\n",
       "      <th>bush</th>\n",
       "      <th>sour</th>\n",
       "      <th>feverish</th>\n",
       "      <th>coincidence</th>\n",
       "      <th>spring</th>\n",
       "      <th>inconvenient</th>\n",
       "      <th>humane</th>\n",
       "      <th>...</th>\n",
       "      <th>ape</th>\n",
       "      <th>insinuation</th>\n",
       "      <th>devolve</th>\n",
       "      <th>answer</th>\n",
       "      <th>advisable</th>\n",
       "      <th>parish</th>\n",
       "      <th>ready</th>\n",
       "      <th>crisis</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(INNOCENCE, INTRODUCTION, Piping, down, the, v...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(wild)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(,, Piping, songs, of, pleasant, glee, ,, On, ...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(\", Pipe, a, song, about, a, Lamb, !, \")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  annuity interminable clearly bush sour feverish coincidence spring  \\\n",
       "0       0            0       0    0    0        0           0      0   \n",
       "1       0            0       0    0    0        0           0      0   \n",
       "2       0            0       0    0    0        0           0      0   \n",
       "3       0            0       0    0    0        0           0      0   \n",
       "4       0            0       0    0    0        0           0      0   \n",
       "\n",
       "  inconvenient humane     ...     ape insinuation devolve answer advisable  \\\n",
       "0            0      0     ...       0           0       0      0         0   \n",
       "1            0      0     ...       0           0       0      0         0   \n",
       "2            0      0     ...       0           0       0      0         0   \n",
       "3            0      0     ...       0           0       0      0         0   \n",
       "4            0      0     ...       0           0       0      0         0   \n",
       "\n",
       "  parish ready crisis                                      text_sentence  \\\n",
       "0      0     0      0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...   \n",
       "1      0     0      0  (INNOCENCE, INTRODUCTION, Piping, down, the, v...   \n",
       "2      0     0      0                                             (wild)   \n",
       "3      0     0      0  (,, Piping, songs, of, pleasant, glee, ,, On, ...   \n",
       "4      0     0      0           (\", Pipe, a, song, about, a, Lamb, !, \")   \n",
       "\n",
       "  text_source  \n",
       "0       Blake  \n",
       "1       Blake  \n",
       "2       Blake  \n",
       "3       Blake  \n",
       "4       Blake  \n",
       "\n",
       "[5 rows x 3014 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts['text_source'].value_counts().head(15).sort_values().plot(kind='barh', figsize=(15,9))\n",
    "plt.title('Number of Sentences')\n",
    "plt.ylabel('Author')\n",
    "plt.xlabel('# of Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW\n",
    "Now let's give the bag of words features a whirl by trying a random forest.\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8860265417642467\n",
      "\n",
      "Test set score: 0.6495327102803738\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "Based on the scores you can tell there is some overfitting going on.\n",
    "Lets try using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1281, 3012) (1281,)\n",
      "Training set score: 0.8477751756440282\n",
      "\n",
      "Test set score: 0.7009345794392523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still getting the same results as the Random forest. Still overfitting.\n",
    "\n",
    "### Lets try with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8071818891491023\n",
      "\n",
      "Test set score: 0.6518691588785047\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results seems a little better. nonetheless still overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "\n",
    "poems_ = gutenberg.paras('blake-poems.txt')\n",
    "stories_ = gutenberg.paras('bryant-stories.txt')\n",
    "sense_ = gutenberg.raw('austen-sense.txt')\n",
    "busterbrown_ = gutenberg.raw('burgess-busterbrown.txt')\n",
    "ball_ = gutenberg.raw('chesterton-ball.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Poems', 'by', 'William', 'Blake', '1789', ']']], [['SONGS', 'OF', 'INNOCENCE', 'AND', 'OF', 'EXPERIENCE', 'and', 'THE', 'BOOK', 'of', 'THEL']], ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the paragraphs from the two novels into one.\n",
    "books = (poems_ + stories_ + sense_ + busterbrown_ + ball_)\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Poems by William Blake 1789 ]', 'SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL', 'SONGS OF INNOCENCE', 'INTRODUCTION']\n"
     ]
    }
   ],
   "source": [
    "#processing\n",
    "books_paras=[]     \n",
    "for paragraph in books:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    books_paras.append(' '.join(para))\n",
    "\n",
    "print(books_paras[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1419\n",
      "Original sentence: i\n",
      "Tf_idf vector: {}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(books_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "books_paras_tfidf=vectorizer.fit_transform(books_paras)\n",
    "print(\"Number of features: %d\" % books_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(books_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the vectors, with one vector per paragraph. Lets do some dimension reduction. We'll use the Singular Value Decomposition (SVD) function from sklearn rather than PCA because we don't want to mean-center our variables (and thus lose sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 53.118042397105\n",
      "Component 0:\n",
      "\" That gives me no idea at all ,\" said the little Jackal .                                                                                              0.771450\n",
      "THE LITTLE COTYLEDONS                                                                                                                                   0.730319\n",
      "\" It is too little , too little .\"                                                                                                                      0.730319\n",
      "\" Now , let us understand the situation ,\" said the little Jackal .                                                                                     0.725460\n",
      "\" Yes ,\" said the little Jackal .                                                                                                                       0.716924\n",
      "\" Oh , I do beg your pardon ,\" said the little Jackal .                                                                                                 0.716612\n",
      "\" Oh , no ,\" said the little girl , \" those are bullfrogs , croaking .\"                                                                                 0.715846\n",
      "\" It is on the other side of the river ,\" said the little Jackal ; \" but we can manage it nicely , if you will take me on your back and swim over .\"    0.711357\n",
      "\" No , it isn ' t ,\" said the little Red Man , \" I have it ; you will never see it again .\"                                                             0.700785\n",
      "\" But I don ' t altogether understand ,\" said the little Jackal .                                                                                       0.697012\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" _Who is there ? _ \" she said .                                                                  0.759434\n",
      "\" It would be a disgrace to you to have it said that one of your subjects dared disobey you !\"    0.752645\n",
      "\" Not I ,\" said the Goose .                                                                       0.723602\n",
      "\" Not I ,\" said the Goose .                                                                       0.723602\n",
      "\" It is Goliath , of Gath , champion of the Philistines ,\" said the soldiers about .              0.614070\n",
      "\" Anything that you will do , I will do ,\" said the tailor .                                      0.591777\n",
      "\" Indeed , no ,\" said father .                                                                    0.551283\n",
      "\" Do it yourself first ,\" said the giant .                                                        0.544077\n",
      "\" You will not ,\" said the giant .                                                                0.544077\n",
      "\" I will ,\" said the Duck .                                                                       0.542068\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Oh , Jackal !\"                                           0.665120\n",
      "Oh !                                                       0.651070\n",
      "Oh !                                                       0.651070\n",
      "\" Oh !                                                     0.651070\n",
      "\" Oh , dear !\"                                             0.553900\n",
      "\" Oh , does it , indeed ?\"                                 0.549481\n",
      "\" Oh , please , dear snow , give me a cap , too !          0.526163\n",
      "\" Oh my children !                                         0.521995\n",
      "\" Oh , I do beg your pardon ,\" said the little Jackal .    0.513334\n",
      "\" Oh , no , no ; you are too little !\"                     0.505843\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" And I can run away from you , I can !                                                                                 0.804921\n",
      "\" And I can run away from you , I can !\"                                                                                0.804921\n",
      "\" And I can run away from you , I can !\"                                                                                0.804921\n",
      "\" And I can run away from you , I can !\"                                                                                0.804921\n",
      "They had run away so many times that they were quite thin and very tired , and they could not run so fast any more .    0.739767\n",
      "\" Run !                                                                                                                 0.640565\n",
      "\" Run !                                                                                                                 0.640565\n",
      "\" Run !                                                                                                                 0.640565\n",
      "\" I have run away from a little old woman ,                                                                             0.599713\n",
      "\" I have run away from a little old woman ,                                                                             0.599713\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "THE FROG KING                                                                                                                                                                                                      0.663795\n",
      "\" Are you not king ?\"                                                                                                                                                                                              0.663795\n",
      "As soon as the Frogs came to the surface to greet the new king , King Stork caught them in his long bill and gobbled them up .                                                                                     0.621260\n",
      "At the splash the Frogs were terribly frightened , and dived into their holes to hide from King Log .                                                                                                              0.616931\n",
      "But when King Saul died , the people chose David for their king , because there was no one so brave , so wise , or so faithful to God .                                                                            0.610963\n",
      "Soon they sent a third messenger to Jove , and begged that they might have a better king , a king who was worth while .                                                                                            0.573292\n",
      "And after the battle , David was taken to the king ' s tent , and made a captain over many men ; and he went no more to his father ' s house , to herd the sheep , but became a man , in the king ' s service .    0.559111\n",
      "The first time that he noticed that even a king could not always have his own way was on a day when he went hunting .                                                                                              0.524800\n",
      "The king on the throne looked at him .                                                                                                                                                                             0.499832\n",
      "\" The very thing for the King ' s dinner !\"                                                                                                                                                                        0.497620\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From gazing at the most representative sample paragraphs, it appears that component 0 targets dialogue around the description 'little', component 1 seems to largely involve dialogue about what was said, component 2 is center around the word Run, component 3 involve dialogue about the character Brahmin , and component 4 involves referencing people as 'dear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
