{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\femis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\femis\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "symbolic link created for C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Linking successful\n",
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this modeling we wou\n",
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "poems = gutenberg.raw('blake-poems.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "poems = re.sub(r'Chapter \\d+', '', poems)\n",
    "stories = re.sub(r'CHAPTER .*', '', stories)\n",
    "    \n",
    "poems = text_cleaner(poems[:int(len(poems)/50)])\n",
    "stories = text_cleaner(stories[:int(len(stories)/50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text:\n",
      " SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping dow\n"
     ]
    }
   ],
   "source": [
    "print('Cleaned text:\\n', poems[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text:\n",
      " TWO LITTLE RIDDLES IN RHYME There's a garden that I ken, Full of little gentlemen; Little caps of bl\n"
     ]
    }
   ],
   "source": [
    "print('Cleaned text:\\n', stories[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "poems_doc = nlp(poems)\n",
    "stories_doc = nlp(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(INNOCENCE, INTRODUCTION, Piping, down, the, v...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(wild)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(,, Piping, songs, of, pleasant, glee, ,, On, ...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\", Pipe, a, song, about, a, Lamb, !, \")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...  Blake\n",
       "1  (INNOCENCE, INTRODUCTION, Piping, down, the, v...  Blake\n",
       "2                                             (wild)  Blake\n",
       "3  (,, Piping, songs, of, pleasant, glee, ,, On, ...  Blake\n",
       "4           (\", Pipe, a, song, about, a, Lamb, !, \")  Blake"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "poems_sents = [[sent, \"Blake\"] for sent in poems_doc.sents]\n",
    "stories_sents = [[sent, \"Bryant\"] for sent in stories_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(poems_sents + stories_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "poemswords = bag_of_words(poems_doc)\n",
    "storieswords = bag_of_words(stories_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(poemswords + storieswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wear</th>\n",
       "      <th>one</th>\n",
       "      <th>mighty</th>\n",
       "      <th>think</th>\n",
       "      <th>valley</th>\n",
       "      <th>garden</th>\n",
       "      <th>springtime</th>\n",
       "      <th>happily</th>\n",
       "      <th>earth</th>\n",
       "      <th>dle</th>\n",
       "      <th>...</th>\n",
       "      <th>pipe</th>\n",
       "      <th>window</th>\n",
       "      <th>early</th>\n",
       "      <th>men</th>\n",
       "      <th>night</th>\n",
       "      <th>soft</th>\n",
       "      <th>shall</th>\n",
       "      <th>easy</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(INNOCENCE, INTRODUCTION, Piping, down, the, v...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(wild)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(,, Piping, songs, of, pleasant, glee, ,, On, ...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(\", Pipe, a, song, about, a, Lamb, !, \")</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 229 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  wear one mighty think valley garden springtime happily earth dle  \\\n",
       "0    0   0      0     0      0      0          0       0     0   0   \n",
       "1    0   0      0     0      1      0          0       0     0   0   \n",
       "2    0   0      0     0      0      0          0       0     0   0   \n",
       "3    0   0      0     0      0      0          0       0     0   0   \n",
       "4    0   0      0     0      0      0          0       0     0   0   \n",
       "\n",
       "      ...     pipe window early men night soft shall easy  \\\n",
       "0     ...        0      0     0   0     0    0     0    0   \n",
       "1     ...        1      0     0   0     0    0     0    0   \n",
       "2     ...        0      0     0   0     0    0     0    0   \n",
       "3     ...        1      0     0   0     0    0     0    0   \n",
       "4     ...        0      0     0   0     0    0     0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...       Blake  \n",
       "1  (INNOCENCE, INTRODUCTION, Piping, down, the, v...       Blake  \n",
       "2                                             (wild)       Blake  \n",
       "3  (,, Piping, songs, of, pleasant, glee, ,, On, ...       Blake  \n",
       "4           (\", Pipe, a, song, about, a, Lamb, !, \")       Blake  \n",
       "\n",
       "[5 rows x 229 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW\n",
    "Now let's give the bag of words features a whirl by trying a random forest.\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9701492537313433\n",
      "\n",
      "Test set score: 0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "Based on the scores you can tell there is some overfitting going on.\n",
    "Lets try using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 227) (67,)\n",
      "Training set score: 0.9701492537313433\n",
      "\n",
      "Test set score: 0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still getting the same results as the Random forest. Still overfitting.\n",
    "\n",
    "### Lets try with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9701492537313433\n",
      "\n",
      "Test set score: 0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results seems a little better. nonetheless still overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "\n",
    "poems_ = gutenberg.paras('blake-poems.txt')\n",
    "stories_ = gutenberg.paras('bryant-stories.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Poems', 'by', 'William', 'Blake', '1789', ']']], [['SONGS', 'OF', 'INNOCENCE', 'AND', 'OF', 'EXPERIENCE', 'and', 'THE', 'BOOK', 'of', 'THEL']], ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the paragraphs from the two novels into one.\n",
    "books = (poems_ + stories_)\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Poems by William Blake 1789 ]', 'SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL', 'SONGS OF INNOCENCE', 'INTRODUCTION']\n"
     ]
    }
   ],
   "source": [
    "#processing\n",
    "books_paras=[]     \n",
    "for paragraph in books:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    books_paras.append(' '.join(para))\n",
    "\n",
    "print(books_paras[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1419\n",
      "Original sentence: Gottlieb was not old enough to work , but he would often sit on a small stool at his mother ' s feet and dream about the wonderful things he would do for his dear mother when he grew to be a man , and she was comforted as she looked upon her boy , and the thought that she was working for him often gave strength to her tired fingers .\n",
      "Tf_idf vector: {'gottlieb': 0.22672671486047277, 'working': 0.25685825384870237, 'fingers': 0.23606474462261776, 'things': 0.22672671486047277, 'looked': 0.181990986965685, 'wonderful': 0.24856696258597494, 'grew': 0.20774609002877253, 'strength': 0.24179248435458756, 'thought': 0.18876546519707235, 'tired': 0.24179248435458756, 'feet': 0.2311031507856148, 'dream': 0.24179248435458756, 'man': 0.1587994354333551, 'dear': 0.1807726234060954, 'small': 0.19186365558436716, 'work': 0.19351533903408252, 'mother': 0.3203014784803656, 'boy': 0.17038415593008738, 'gave': 0.2030954332319343, 'old': 0.15749555570375864, 'sit': 0.20774609002877253}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(books_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "books_paras_tfidf=vectorizer.fit_transform(books_paras)\n",
    "print(\"Number of features: %d\" % books_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(books_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the vectors, with one vector per paragraph. Lets do some dimension reduction. We'll use the Singular Value Decomposition (SVD) function from sklearn rather than PCA because we don't want to mean-center our variables (and thus lose sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 48.77275660105181\n",
      "Component 0:\n",
      "\" There are none little enough ,\" they said .                                                                                                           0.851323\n",
      "TWO LITTLE RIDDLES IN RHYME                                                                                                                             0.698295\n",
      "THE LITTLE COTYLEDONS                                                                                                                                   0.698295\n",
      "THE LITTLE VAGABOND                                                                                                                                     0.640456\n",
      "\" It is on the other side of the river ,\" said the little Jackal ; \" but we can manage it nicely , if you will take me on your back and swim over .\"    0.637163\n",
      "\" But I don ' t altogether understand ,\" said the little Jackal .                                                                                       0.618953\n",
      "\" Oh , no ; you are too little , you are too little !\"                                                                                                  0.612650\n",
      "\" Oh no , no , no ; you are too little , you are too little !\"                                                                                          0.612650\n",
      "\" Oh ,\" said the little Jackal , \" you want my opinion ?                                                                                                0.581574\n",
      "\" Yes ,\" said the little Jackal .                                                                                                                       0.579687\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" Now you do that ,\" said he .                                                                                                                      0.704587\n",
      "\" _Who is there ? _ \" she said .                                                                                                                    0.704587\n",
      "\" Not I ,\" said the Goose .                                                                                                                         0.671992\n",
      "\" Anything that you will do , I will do ,\" said the tailor .                                                                                        0.518201\n",
      "The fourth said that he considered the tail greatly improved .                                                                                      0.503434\n",
      "\" I said ,\" said the Brahmin , raising his voice , \" do you think it is fair that the Tiger should eat me , when I set him free from his cage ?\"    0.460887\n",
      "\" Who is there ?\"                                                                                                                                   0.435817\n",
      "So he put on a very cheerful voice , as if nothing at all were the matter , and he said ,                                                           0.429513\n",
      "\" I will not ,\" said the tailor ; \" remain where you are until I come back , and I ' ll lift you up .\"                                              0.411318\n",
      "\" Why , the cage he was in ,\" said the Brahmin .                                                                                                    0.410776\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" And I can run away from you , I can !\"                                                                                0.884505\n",
      "\" And I can run away from you , I can !\"                                                                                0.884505\n",
      "\" Run !                                                                                                                 0.808572\n",
      "\" Run !                                                                                                                 0.808572\n",
      "\" Run !                                                                                                                 0.808572\n",
      "\" Run !                                                                                                                 0.808572\n",
      "\" Run !                                                                                                                 0.808572\n",
      "They had run away so many times that they were quite thin and very tired , and they could not run so fast any more .    0.786062\n",
      "\" I have run away from a little old woman ,                                                                             0.772991\n",
      "\" I have run away from a little old woman ,                                                                             0.772991\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" Oh , Brother Brahmin , Brother Brahmin ,\" said the Tiger , \" please let me out , to get a little drink !                                                   0.637313\n",
      "THE BRAHMIN , THE TIGER , AND THE JACKAL                                                                                                                     0.614635\n",
      "\" Oh , Brother Jackal , dear Brother Jackal ,\" said the Brahmin , \" give us your opinion !                                                                   0.608660\n",
      "\" But , Brother Tiger ,\" said the Brahmin , \" you promised you would not .                                                                                   0.568029\n",
      "So the Brahmin , the Tiger , and the little Jackal walked back together to the spot where the cage was .                                                     0.567513\n",
      "The Tiger sprang to eat the Brahmin , but the Brahmin said ,                                                                                                 0.558665\n",
      "\" Brother Bullock , oh , Brother Bullock , does it seem to you a fair thing that this Tiger should eat me up , after I have just freed him from a cage ?\"    0.554858\n",
      "\" Never , Brother Brahmin !\"                                                                                                                                 0.535863\n",
      "The Tiger sprang , but the Brahmin spoke very quickly ,                                                                                                      0.514451\n",
      "\" Brother Banyan ,\" said the Brahmin , eagerly , \" does it seem to you right or just that this Tiger should eat me , when I set him free from his cage ?\"    0.504303\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" Oh , dear , dear !\"                                                                                         0.703652\n",
      "\" Oh , dear !\"                                                                                                0.698095\n",
      "Dear me !                                                                                                     0.605500\n",
      "\" Dear me !\"                                                                                                  0.605500\n",
      "\" Dear me !\"                                                                                                  0.605500\n",
      "Dear mother , dear mother , the Church is cold ; But the Alehouse is healthy , and pleasant , and warm .      0.540209\n",
      "In the middle of the stream the fox said , \" Oh , dear !                                                      0.525038\n",
      "\" Oh , please , dear , strong Mr Whale ,\" he said , \" will you have the great kindness to do me a favour ?    0.516187\n",
      "\" Now stretch out your little useless fingers , dear !\"                                                       0.392065\n",
      "Oh !                                                                                                          0.374923\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From gazing at the most representative sample paragraphs, it appears that component 0 targets dialogue around the description 'little', component 1 seems to largely involve dialogue about what was said, component 2 is center around the word Run, component 3 involve dialogue about the character Brahmin , and component 4 involves referencing people as 'dear'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how similar various sentences are to one another. For example, here are the similarity scores (as a heatmap) of the first 10 sentences in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 \" Who is there ?\"\n",
      "1 The Cloud descended and the Lily bowd her modest head : And went to mind her numerous charge among the verdant grass .\n",
      "2 I give this explanation that the reader may know I do not presume to offer the little tale which follows as an \" adaptation \" of Andersen ' s famous story .\n",
      "3 \" You ?\"\n",
      "4 They dug until they had turned up the soil from one end of the orchard to the other , round the tree - roots and between them .\n",
      "5 Gottlieb was not old enough to work , but he would often sit on a small stool at his mother ' s feet and dream about the wonderful things he would do for his dear mother when he grew to be a man , and she was comforted as she looked upon her boy , and the thought that she was working for him often gave strength to her tired fingers .\n",
      "6 A little farther out , the fox said , \" I am afraid the water will cover you , there ; jump on my shoulder .\"\n",
      "7 \" But ,\" said David , \" who is this Philistine , that he should defy the armies of the living God ?\"\n",
      "8 \" Run !\n",
      "9 Didst close my tongue in senseless clay , And me to mortal life betray .\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
